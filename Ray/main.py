# coding: utf-8

# Author: Du Mingzhe (mingzhe@nus.edu.sg)
# Date: 2025-05-27

import re
import os
import json
import string
import random
import shutil
import tempfile
import datetime
import argparse
import subprocess
from tqdm import tqdm
from collections import defaultdict
from tqdm.contrib.concurrent import process_map

toml_template = """
[cosmic-ray]
module-path = "mod.py"
timeout = {timeout}
excluded-modules = []
test-command = "pytest test.py"

[cosmic-ray.distributor]
name = "local"
"""

code_import = """
import os
import re
import math
import numpy
import pandas
import pytest
import random
import string
import warnings
import datetime
import traceback
import numpy as np
import pandas as pd
from typing import List, Dict, Any, Optional, Union, Tuple, Set, FrozenSet, Sequence, Iterable, Generator, Callable
"""

def rename_test_functions(test_code):
    test_counter = 0
    new_lines = list()
    pattern = re.compile(r"(\s*def\s+)(test_)(.*)")
    test_code_lines = test_code.split('\n')
    
    for line in test_code_lines:
        match = pattern.match(line)
        if match:
            test_counter += 1
            new_lines.append(f"{match.group(1)}test_{test_counter}_{match.group(3)}")
        else:
            new_lines.append(line)
    
    return '\n'.join(new_lines)

def parse_pytest_output(output: str) -> dict:
    """
    Parses the stdout of a `pytest --cov` run to extract key metrics.

    Args:
        output: The string output from the pytest command.

    Returns:
        A dictionary containing test results and coverage.
    """
    # ğŸ¯ Default values
    total_coverage = 0
    passed_count = 0
    failed_count = 0

    # Regex to find the total coverage percentage from the 'TOTAL' line
    coverage_match = re.search(r"^TOTAL\s+.*\s+(\d+)%$", output, re.MULTILINE)
    if coverage_match:
        total_coverage = int(coverage_match.group(1))

    # Regex to find the numbers in the final summary line
    # e.g., "===... 4 failed, 1 passed in 9.85s ...==="
    summary_line_match = re.search(
        r"=+ (.*) in .*s =+", output
    )
    if summary_line_match:
        summary_text = summary_line_match.group(1)
        
        passed_match = re.search(r"(\d+)\s+passed", summary_text)
        if passed_match:
            passed_count = int(passed_match.group(1))
            
        failed_match = re.search(r"(\d+)\s+failed", summary_text)
        if failed_match:
            failed_count = int(failed_match.group(1))

    # --- Calculations ---
    total_tests = passed_count + failed_count
    pass_rate = 0.0
    if total_tests > 0:
        pass_rate = (passed_count / total_tests) * 100

    return {
        "total_coverage_percent": total_coverage,
        "pass_rate_percent": round(pass_rate, 2),
        "passed_tests": passed_count,
        "failed_tests": failed_count,
        "total_tests": total_tests,
    }

# Initialization 
def cosmic_ray_init(benchmark_name, model_name, model_generation_file, num_test_cases=5, timeout=1, num_samples=100):
    if os.path.exists(f'data/{benchmark_name}/mutation_{num_test_cases}/{model_name}'):
        print(f"[+] ğŸ§¹ Cleaning up existing files in {model_name}...")
        try:
            shutil.rmtree(f'data/{benchmark_name}/mutation_{num_test_cases}/{model_name}')
        except PermissionError:
            print(f"[-] PermissionError: {f'data/{benchmark_name}/mutation_{num_test_cases}/{model_name}'}")
        
    print(f"[+] ğŸ“‚ Creating new directory {model_name}...")
    os.makedirs(f'data/{benchmark_name}/mutation_{num_test_cases}/{model_name}')

    with open(model_generation_file, 'r') as data_handler:
        # For [json] file
        # raw_data = json.loads(data_handler.read())[:num_samples]

        # For [jsonl] file
        raw_data = [json.loads(line) for line in data_handler.readlines()[:num_samples]]
    print(f"[+] âœ… Raw data: {len(raw_data)}")

    for idx, instance in tqdm(enumerate(raw_data), desc="[+] ğŸ’¾ Processing raw data"):
        os.makedirs(f'data/{benchmark_name}/mutation_{num_test_cases}/{model_name}/task_{idx}')

        # create 'mod.py'
        with open(f'data/{benchmark_name}/mutation_{num_test_cases}/{model_name}/task_{idx}/mod.py', 'w') as f:
            mod_code = ''
            mod_code += code_import + '\n\n'
            mod_code += instance['code'] + '\n\n'
            mod_code = rename_test_functions(mod_code)
            f.write(mod_code)

        # create 'test.py'
        with open(f'data/{benchmark_name}/mutation_{num_test_cases}/{model_name}/task_{idx}/test.py', 'w') as f:
            test_code = code_import + '\n\n' + 'from mod import *' + '\n\n'
            for test in instance['tests'][:num_test_cases]:
                test_code += f'{test}\n\n'
            # test_code += "\n\n" + "#" * 100 + "\n\n"
            f.write(test_code)         

        # create 'toml'
        with open(f'data/{benchmark_name}/mutation_{num_test_cases}/{model_name}/task_{idx}/cosmic-ray.toml', 'w') as f:
            f.write(toml_template.format(model_name=model_name, task_id=idx, timeout=timeout))

def cosmic_ray_setup_wrapper(benchmark_name, model_name, task_id, num_test_cases=5):
    working_dir = f'data/{benchmark_name}/mutation_{num_test_cases}/{model_name}/{task_id}'
    
    # Initialize Cosmic-Ray Config
    try:
        subprocess.run(['cosmic-ray', 'init', 'cosmic-ray.toml', 'cosmic-ray.sqlite'], cwd=working_dir, check=True)
    except Exception as e:
        print(f'[-] Initialize Cosmic-Ray Error: {e}')
        return False

    # Run Cosmic-Ray Baseline
    try:
        subprocess.run(['cosmic-ray', 'baseline', 'cosmic-ray.toml'], cwd=working_dir, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, timeout=60*num_test_cases)
        return True
    except Exception as e:
        return False

def cosmic_ray_setup(benchmark_name, model_name, num_test_cases=5, sample_rate=0.1):
    # å®šä¹‰è¾“å‡ºæ–‡ä»¶è·¯å¾„
    correct_tasks_path = f'data/{benchmark_name}/correct_tasks_tc_{num_test_cases}_{model_name}'

    # --- 1. è·å–æ‰€æœ‰é€šè¿‡ Pytest çš„ä»»åŠ¡ (å€™é€‰è€…) ---
    pytest_result_file = f'data/{benchmark_name}/mutation_{num_test_cases}/{model_name}/{model_name}_{num_test_cases}.json'
    passed_tasks_current_run = []
    
    # å°è¯•ä» Pytest ç»“æœä¸­è¯»å–
    if os.path.exists(pytest_result_file):
        print(f"[+] ğŸ“– Reading pytest results from {pytest_result_file}")
        with open(pytest_result_file, 'r') as f:
            pytest_data = json.load(f)
        
        for entry in pytest_data:
            # ç¡®ä¿ entry åŒ…å«å¿…è¦å­—æ®µ
            try:
                task_id = entry.get('task_id')
                # æ£€æŸ¥ç‰¹å®š k ä¸‹çš„é€šè¿‡æƒ…å†µ
                test_counts = entry['test_at_k'][f"test@{num_test_cases}"]['result'][0]['test_counts']
                if test_counts['passed_tests'] == test_counts['total_tests'] and test_counts['total_tests'] > 0:
                    passed_tasks_current_run.append(task_id)
            except:
                continue
    else:
        raise Exception(f"[-] âš ï¸ Pytest results not found! Please check {pytest_result_file}.")

    # --- 2. è·å–æˆ–ç”Ÿæˆå›ºå®šæŠ½æ ·åå• (ç™½åå•) ---
    # æ–‡ä»¶åè·Ÿ Modelï¼Œnum_test_cases æ— å…³ -> ä¿è¯ k=5 å’Œ k=1 ä½¿ç”¨åŒä¸€ä¸ªæŠ½æ ·æ± 
    fixed_sample_file = f'data/{benchmark_name}/fixed_sample_{int(sample_rate*100)}pct.json'
    target_sample_pool = []

    if os.path.exists(fixed_sample_file):
        # A. åŠ è½½å·²æœ‰åå•
        print(f"[+] ğŸ“œ Loading fixed sample pool: {fixed_sample_file}")
        with open(fixed_sample_file, 'r') as f:
            target_sample_pool = json.load(f)
    else:
        # B. é¦–æ¬¡è¿è¡Œï¼Œç”Ÿæˆåå•
        print(f"[+] ğŸ² Generating NEW fixed sample pool...")
        working_dir = f'data/{benchmark_name}/mutation_{num_test_cases}/{model_name}'
        # è·å–ç›®å½•ä¸‹æ‰€æœ‰ä»»åŠ¡ä½œä¸ºåŸºåº•
        all_possible_tasks = [t for t in os.listdir(working_dir) if t.startswith('task_')]
        
        if sample_rate < 1.0:
            random.seed(42) # å›ºå®šç§å­
            sample_size = int(len(all_possible_tasks) * sample_rate)
            if sample_size == 0 and len(all_possible_tasks) > 0: sample_size = 1
            target_sample_pool = random.sample(all_possible_tasks, sample_size)
        else:
            target_sample_pool = all_possible_tasks
            
        with open(fixed_sample_file, 'w') as f:
            json.dump(target_sample_pool, f)
        print(f"[+] ğŸ’¾ Saved fixed sample pool ({len(target_sample_pool)} tasks)")

    # --- 3. è®¡ç®—äº¤é›†ï¼šå³å°†åœ¨æœ¬æ¬¡è¿è¡Œ Setup çš„ä»»åŠ¡ ---
    # é€»è¾‘ï¼šå¿…é¡»åœ¨ç™½åå•é‡Œ AND å¿…é¡»é€šè¿‡äº†å½“å‰çš„æµ‹è¯•
    tasks_to_setup = list(set(target_sample_pool) & set(passed_tasks_current_run))
    
    # ç®€å•çš„æ’åºä¿è¯æ—¥å¿—å¯è¯»æ€§
    try:
        tasks_to_setup.sort(key=lambda x: int(x.split('_')[1]))
    except:
        tasks_to_setup.sort()

    print(f"[+] ğŸš€ Tasks selected for mutation: {len(tasks_to_setup)} (Pool: {len(target_sample_pool)} | Passed: {len(passed_tasks_current_run)})")

    if not tasks_to_setup:
        print("[-] No tasks to setup.")
        with open(correct_tasks_path, 'w') as f: pass # åˆ›å»ºç©ºæ–‡ä»¶é˜²æ­¢æŠ¥é”™
        return

    # --- 4. è¿è¡Œ Setup (åªé’ˆå¯¹ç­›é€‰åçš„ä»»åŠ¡) ---
    task_results = process_map(
        cosmic_ray_setup_wrapper, 
        [benchmark_name]*len(tasks_to_setup), 
        [model_name]*len(tasks_to_setup), 
        tasks_to_setup, 
        [num_test_cases]*len(tasks_to_setup), 
        desc="[+] ğŸ”„ Initialize Cosmic-Ray Mutation", 
        chunksize=1
    )
    
    # --- 5. ä¿å­˜ç»“æœåˆ° correct_tasks æ–‡ä»¶ ---
    # mutation_run å’Œ mutation_statistic ä¼šç›´æ¥è¯»å–è¿™ä¸ªæ–‡ä»¶ï¼Œæ‰€ä»¥å®ƒä»¬åªä¼šåœ¨è¿™äº›ä»»åŠ¡ä¸Šè¿è¡Œ
    correct_count = 0
    with open(correct_tasks_path, 'w') as f:
        for task_id, result in zip(tasks_to_setup, task_results):
            if result:
                f.write(f'{task_id}\n')
                correct_count += 1

    print(f'[+] âœ… Setup finished. Valid tasks ready for mutation: {correct_count}')

def cosmic_ray_status(benchmark_name, model_name, task, num_test_cases):
    try:
        cosmic_ray_path = f'data/{benchmark_name}/mutation_{num_test_cases}/{model_name}/{task}/cosmic-ray.sqlite'
        response = subprocess.run(['cr-report', cosmic_ray_path, '--show-pending'], check=True, capture_output=True, text=True)
    except Exception as e:
        print(f'[-] Error @ [{cosmic_ray_path}]: {e}')
        return (False, 0, 0)
    
    total_jobs_match = re.search(r"total jobs:\s*(\d+)", response.stdout)
    completed_jobs_match = re.search(r"complete:\s*(\d+)\s*\(", response.stdout)

    if total_jobs_match and completed_jobs_match:
        total_jobs_number = int(total_jobs_match.group(1))
        completed_jobs_number = int(completed_jobs_match.group(1))
        # print(f"[+] Task {task}: Total jobs: {total_jobs_number}, Completed jobs: {completed_jobs_number}")
        if total_jobs_number == 0: return (True, 0, 0)
        return (completed_jobs_number == total_jobs_number, total_jobs_number, completed_jobs_number)
    else:
        return (False, 0, 0)
    
def mutation_status(benchmark_name, model_name, num_test_cases):
    correct_tasks = list()
    correct_tasks_path = f'data/{benchmark_name}/correct_tasks_tc_{num_test_cases}_{model_name}'
    
    with open(correct_tasks_path, 'r') as f:
        for line in f.readlines():
            correct_tasks.append(line.strip())
    print(f'[+] âœ… Correct Tasks: {len(correct_tasks)}')
    
    for task in correct_tasks:
        completed, total_jobs_number, completed_jobs_number = cosmic_ray_status(benchmark_name, model_name, task, num_test_cases)
        if completed: 
            print(f'[+] Task {task}: Completed ({completed_jobs_number}/{total_jobs_number})')
        else: 
            print(f'[-] Task {task}: Incompleted ({completed_jobs_number}/{total_jobs_number})')

def mutation_run_wrapper(benchmark_name, model_name, num_test_cases, task):
    # cosmic-ray exec tutorial.toml tutorial.sqlite
    completed, _, _ = cosmic_ray_status(benchmark_name, model_name, task, num_test_cases)
    if completed: return

    # print(f"[+] Task {task}: Running mutations")
    working_dir = f'data/{benchmark_name}/mutation_{num_test_cases}/{model_name}/{task}'
    try:
        subprocess.run(['cosmic-ray', 'exec', f'cosmic-ray.toml', f'cosmic-ray.sqlite'], cwd=working_dir, check=True, timeout=360*num_test_cases)
    except subprocess.TimeoutExpired as e:
        # print(f'[-] mutation_run_wrapper, Timeout: {e}')
        pass
    except Exception as e:
        print(f'[-] mutation_run_wrapper, Error: {e}')

def mutation_run(benchmark_name, model_name, num_test_cases):
    correct_tasks = list()
    correct_tasks_path = f'data/{benchmark_name}/correct_tasks_tc_5_{model_name}'
    
    with open(correct_tasks_path, 'r') as f:
        for line in f.readlines():
            correct_tasks.append(line.strip())
    print(f'[+] âœ… Correct Tasks: {len(correct_tasks)}')

    print("================================================")
    print(f'[+] â±ï¸ Start time: {datetime.datetime.now()}')
    process_map(mutation_run_wrapper, [benchmark_name]*len(correct_tasks), [model_name]*len(correct_tasks), [num_test_cases]*len(correct_tasks), correct_tasks, desc="[+] ğŸ”® Running mutations...")
    print(f'[+] â±ï¸ End time: {datetime.datetime.now()}')

def mutation_statistic_wrapper(benchmark_name, model_name, num_test_cases, task):
    working_dir = f'data/{benchmark_name}/mutation_{num_test_cases}/{model_name}/{task}'

    statistic_info = {
        "task": task,
        "complete_rate": 0.0,
        "surviving_mutants_rate": 0.0,
        "total_jobs_number": 0,
        "completed_jobs_number": 0,
        "surviving_mutants_number": 0
    }

    try:
        response = subprocess.run(['cr-report', f'cosmic-ray.sqlite', '--show-pending'], cwd=working_dir, check=True, capture_output=True, text=True)
    except Exception as e:
        print(f'[-] Error @ [{working_dir}]: {e}')
        return statistic_info

    total_jobs_match = re.search(r"total jobs:\s*(\d+)", response.stdout)
    completed_jobs_match = re.search(r"complete:\s*(\d+)\s*\(", response.stdout)
    surviving_mutants_match = re.search(r"surviving mutants:\s*(\d+)\s*\(", response.stdout)

    if total_jobs_match:
        total_jobs_number = int(total_jobs_match.group(1))
        statistic_info["total_jobs_number"] = total_jobs_number

    if completed_jobs_match:
        completed_jobs_number = int(completed_jobs_match.group(1))
        statistic_info["completed_jobs_number"] = completed_jobs_number
        
    if surviving_mutants_match:
        surviving_mutants_number = int(surviving_mutants_match.group(1))
        statistic_info["surviving_mutants_number"] = surviving_mutants_number
    
    statistic_info['complete_rate'] = statistic_info['completed_jobs_number'] / statistic_info["total_jobs_number"] if statistic_info["total_jobs_number"] > 0 else 0
    statistic_info['surviving_mutants_rate'] = (statistic_info['surviving_mutants_number'] / statistic_info['completed_jobs_number']) if statistic_info['completed_jobs_number'] > 0 else 0

    return statistic_info

def mutation_statistic(benchmark_name, model_name, num_test_cases, baseline_test_cases=5):
    correct_tasks = list()
    correct_tasks_path = f'data/{benchmark_name}/correct_tasks_tc_{baseline_test_cases}_{model_name}'
    
    with open(correct_tasks_path, 'r') as f:
        for line in f.readlines():  
            correct_tasks.append(line.strip())
    print(f'[+] âœ… Correct Tasks: {len(correct_tasks)}')
    
    surviving_mutants_rate = 0.0

    statistics = process_map(mutation_statistic_wrapper, [benchmark_name]*len(correct_tasks), [model_name]*len(correct_tasks), [num_test_cases]*len(correct_tasks), correct_tasks, desc=f"[+] ğŸ”„ Running mutation ({num_test_cases} test cases) statistics...", chunksize=1)
    for statistic in statistics:
        print(f"[+] {statistic}")
        surviving_mutants_rate += statistic["surviving_mutants_rate"]
    
    surviving_mutants_rate = (surviving_mutants_rate / len(correct_tasks)) if len(correct_tasks) > 0 else 0.0
    print(f'[+] âœ… Surviving Mutants Rate: {surviving_mutants_rate:.2%} \n')

    return surviving_mutants_rate

def pytest_run_wrapper(benchmark_name, model_name, task_id, num_test_cases):
    base_dir = f'data/{benchmark_name}/mutation_{num_test_cases}/{model_name}/{task_id}'
    test_file_path = f'{base_dir}/test.py'
    source_code_path = base_dir
    
    # é»˜è®¤ç©ºæ•°æ®ç»“æ„ï¼Œé˜²æ­¢æŠ¥é”™
    empty_stats = [
        {"test_counts": {"passed_tests": 0, "total_tests": num_test_cases}},
        {"stmts": 0, "miss_stmts": 0, "covered_branches": 0, "total_branches": 0}
    ]

    try:
        with tempfile.TemporaryDirectory() as temp_dir:
            abs_test_file_path = os.path.abspath(test_file_path)
            abs_source_code_path = os.path.abspath(source_code_path)
            json_report_path = os.path.join(temp_dir, "coverage.json")

            coveragerc_path = os.path.join(temp_dir, ".coveragerc")
            with open(coveragerc_path, "w") as f:
                f.write("[run]\n")
                f.write("omit = *test.py\n") # å¿½ç•¥æ‰€æœ‰ä»¥ test.py ç»“å°¾çš„æ–‡ä»¶

            # 1. è¿è¡Œ Pytest å¹¶ç”Ÿæˆ JSON æŠ¥å‘Š
            cmd = [
                'pytest', 
                abs_test_file_path, 
                f'--cov={abs_source_code_path}', 
                '--cov-branch',
                f'--cov-config={coveragerc_path}', # æŒ‡å®šé…ç½®æ–‡ä»¶
                f'--cov-report=json:{json_report_path}'
            ]
            
            result = subprocess.run(cmd, cwd=temp_dir, capture_output=True, text=True, timeout=30)
            
            # 2. è·å–é€šè¿‡ç”¨ä¾‹æ•° (result[0])
            # ä¾ç„¶ä½¿ç”¨ parse_pytest_output è§£æ stdout æ¥è·å– passed/failed æ•°é‡
            # å› ä¸º coverage.json é‡Œé€šå¸¸ä¸åŒ…å«å…·ä½“çš„æµ‹è¯•é€šè¿‡æ•°
            stdout_metrics = parse_pytest_output(result.stdout)
            passed_tests = stdout_metrics.get("passed_tests", 0)
            total_tests_run = stdout_metrics.get("total_tests", 0)

            # 3. è·å–è¦†ç›–ç‡è¯¦æƒ… (result[1])
            stmts = 0
            miss_stmts = 0
            covered_branches = 0
            total_branches = 0

            if os.path.exists(json_report_path):
                with open(json_report_path, 'r') as f:
                    cov_data = json.load(f)
                totals = cov_data.get('totals', {})
                
                stmts = totals.get('num_statements', 0)
                covered_lines = totals.get('covered_lines', 0)
                miss_stmts = stmts - covered_lines # ç»Ÿè®¡è„šæœ¬éœ€è¦ miss_stmts
                
                covered_branches = totals.get('covered_branches', 0)
                total_branches = totals.get('num_branches', 0)

            # 4. æ„å»ºç¬¦åˆç»Ÿè®¡è„šæœ¬è¦æ±‚çš„æ ¼å¼
            formatted_result = [
                {
                    "test_counts": {
                        "passed_tests": passed_tests,
                        "total_tests": total_tests_run
                    }
                },
                {
                    "stmts": stmts,
                    "miss_stmts": miss_stmts,
                    "covered_branches": covered_branches,
                    "total_branches": total_branches
                }
            ]

            return {
                'model_name': model_name, 
                'task': task_id, 
                'test_at_k_data': formatted_result, # å°†æ ¼å¼åŒ–å¥½çš„æ•°æ®ä¼ å‡ºå»
                "status": "success"
            }
            
    except Exception as e:
        print(f"[-] Error in task {task_id}: {e}")
        return {
            'model_name': model_name, 
            'task': task_id, 
            'test_at_k_data': empty_stats, 
            "status": "error"
        }

def pytest_run(benchmark_name, model_name, num_test_cases):
    tasks = list()
    work_dir = f'data/{benchmark_name}/mutation_{num_test_cases}/{model_name}'
    
    if not os.path.exists(work_dir):
        print(f"[-] Directory not found: {work_dir}")
        return

    # è·å–æ‰€æœ‰ä»»åŠ¡æ–‡ä»¶å¤¹
    # æŒ‰ç…§ task_0, task_1, task_2 ... æ’åºï¼Œä¿è¯é¡ºåºä¸ dataset ä¸€è‡´ï¼ˆå¦‚æœç»Ÿè®¡è„šæœ¬ä¾èµ–é¡ºåºï¼‰
    # è¿™é‡Œåšä¸€ä¸ªç®€å•çš„æ ¹æ®æ•°å­—æ’åº
    all_files = os.listdir(work_dir)
    task_files = [t for t in all_files if t.startswith('task_')]
    
    # å°è¯•æŒ‰æ•°å­—æ’åº task_0 -> 0
    try:
        task_files.sort(key=lambda x: int(x.split('_')[1]))
    except:
        task_files.sort()

    tasks = task_files
            
    results = process_map(pytest_run_wrapper, 
                          [benchmark_name]*len(tasks), 
                          [model_name]*len(tasks), 
                          tasks, 
                          [num_test_cases]*len(tasks), 
                          desc="[+] ğŸ”„ Running pytest", 
                          chunksize=1)
    
    # --- æ ¸å¿ƒä¿®æ”¹ï¼šæ„å»ºæœ€ç»ˆçš„å¤§åˆ—è¡¨ ---
    final_output_list = []
    
    for res in results:
        # æ„å»ºç»Ÿè®¡è„šæœ¬ analyze_test_at_k_results éœ€è¦çš„ entry ç»“æ„
        # ç»“æ„: entry["test_at_k"][f"test@{k}"]["result"]...
        
        # è¿™é‡Œçš„ key æ˜¯ "test@5" (å¦‚æœ num_test_cases=5)
        k_key = f"test@{num_test_cases}"
        
        entry = {
            "task_id": res['task'], # ä¿ç•™ task_id æ–¹ä¾¿è°ƒè¯•
            "test_at_k": {
                k_key: {
                    "result": res['test_at_k_data']
                }
            }
        }
        final_output_list.append(entry)
    
    # --- å†™å…¥æ–‡ä»¶ ---
    # æ³¨æ„ï¼šç»Ÿè®¡è„šæœ¬ç”¨ json.load(f) è¯»å–ï¼Œæ‰€ä»¥è¿™é‡Œå¿…é¡» dump æ•´ä¸ªåˆ—è¡¨ï¼Œä¸èƒ½æ˜¯ jsonl
    output_file = f'{work_dir}/{model_name}_{num_test_cases}.json' 
    # æˆ‘æŠŠåç¼€æ”¹æˆäº† .json ä»¥ç¤ºåŒºåˆ«ï¼Œä½†ä½ çš„ç»Ÿè®¡è„šæœ¬é‡Œå¯èƒ½æ˜¯ .jsonlï¼Œè¯·æ³¨æ„å¯¹åº”
    
    with open(output_file, 'w') as f:
        json.dump(final_output_list, f, indent=2)
        
    print(f"[+] âœ… Pytest results saved to {output_file}")

def merge_k_results(benchmark_name, model_name, k_values_list):
    print(f"[+] ğŸ”— Merging results for {model_name} with k={k_values_list}...")
    
    # ä½¿ç”¨å­—å…¸æŒ‰ task_id èšåˆæ•°æ®
    # ç»“æ„: merged_data["task_0"]["test_at_k"]["test@5"] = ...
    merged_data = defaultdict(lambda: {"task_id": "", "test_at_k": {}})
    
    files_found = 0

    for k in k_values_list:
        # å¯¹åº” pytest_run ä¸­ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„
        part_file = f'data/{benchmark_name}/mutation_{k}/{model_name}/{model_name}_{k}.json'
        
        if os.path.exists(part_file):
            files_found += 1
            with open(part_file, 'r') as f:
                data = json.load(f)
                
            for entry in data:
                t_id = entry['task_id']
                # ç¡®ä¿ task_id å­—æ®µå­˜åœ¨
                merged_data[t_id]['task_id'] = t_id
                
                # å°†å½“å‰çš„ test@k æ•°æ®æ›´æ–°åˆ°å¤§å­—å…¸ä¸­
                # entry['test_at_k'] ç±»ä¼¼äº {"test@5": {...}}
                merged_data[t_id]['test_at_k'].update(entry['test_at_k'])
        else:
            print(f"[-] âš ï¸ Warning: Partial result file not found: {part_file}")

    if files_found == 0:
        print(f"[-] No files found to merge for {model_name}")
        return

    # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æ’åº
    final_list = list(merged_data.values())
    
    # æŒ‰ task_id æ•°å­—æ’åº (task_0, task_1...)
    try:
        final_list.sort(key=lambda x: int(x['task_id'].split('_')[1]))
    except:
        final_list.sort(key=lambda x: x['task_id'])

    # ä¿å­˜æœ€ç»ˆåˆå¹¶æ–‡ä»¶
    # å»ºè®®ä¿å­˜åœ¨ benchmark æ ¹ç›®å½•ä¸‹ï¼Œæˆ–è€…ä½ æŒ‡å®šçš„ results ç›®å½•
    output_dir = f'data/{benchmark_name}/pytest_results'
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        
    final_output_path = f'{output_dir}/{model_name}.json'
    
    with open(final_output_path, 'w') as f:
        json.dump(final_list, f, indent=2)
        
    print(f"[+] ğŸ‰ Successfully merged {files_found} files into: {final_output_path}")

if __name__ == "__main__":    
    parser = argparse.ArgumentParser()
    parser.add_argument("--benchmark_name", type=str, default='ULT')
    parser.add_argument("--num_samples", type=int, default=10000)
    parser.add_argument("--mode", type=str, default='all')
    args = parser.parse_args()
    
    with open('models.txt', 'r', encoding='utf-8') as f:
        model_list = f.read().splitlines()
    models = [model.split('/')[-1] for model in model_list]

    for num_test_cases in [5,2,1]:
        for model_name in models:
            cosmic_ray_init(args.benchmark_name, model_name, f'src/results/{model_name}_format.jsonl', timeout=10, num_samples=args.num_samples, num_test_cases=num_test_cases)
            pytest_run(args.benchmark_name, model_name, num_test_cases)
            cosmic_ray_setup(args.benchmark_name, model_name, num_test_cases=num_test_cases)
            mutation_status(args.benchmark_name, model_name, num_test_cases=num_test_cases)
            mutation_run(args.benchmark_name, model_name, num_test_cases)
            # mutation_statistic(args.benchmark_name, model_generation_file_path, num_test_cases, baseline_test_cases=5)

    for model_name in models:
        merge_k_results(args.benchmark_name, model_name, [5,2,1])
